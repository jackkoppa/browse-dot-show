# Orama Streaming Persistence Implementation

## Problem
- Orama's built-in persistence plugin has a 512MB limit due to JavaScript string length limits
- Current approach fails with "Invalid string length" error when serializing large indexes
- Our index has 380K+ search entries (~5GB in memory) which exceeds the string limit

## Solution: Streaming Persistence with MsgPack
Based on prototype from: https://github.com/oramasearch/orama/issues/851#issuecomment-2888461388

### Implementation Plan

#### Phase 1: Setup Dependencies âœ…
- âœ… Checked @msgpack/msgpack availability (v3.1.2 installed)
- âœ… Added compression support (gzip only - zstd requires external package)

#### Phase 2: Create Streaming Persistence Functions âœ…
- âœ… `persistToFileStreaming()` - replaces `serializeOramaIndex()`
- âœ… `restoreFromFileStreaming()` - for reading back the index
- âœ… Support compression options: none, gzip (zstd removed due to Node.js limitations)

#### Phase 3: Update Database Module âœ…
- âœ… Replace `serializeOramaIndex()` with streaming approach
- âœ… Use Orama's `save()` function instead of `persist()`
- âœ… Return file path instead of Buffer
- âœ… Export new functions and types

#### Phase 4: Update SRT Indexing Lambda âœ…
- âœ… Modify to use new streaming persistence
- âœ… Update file handling logic for S3 upload
- âœ… Build completed successfully

#### Phase 5: Testing (READY FOR USER TO TEST)
- ðŸ”„ Test with full myfavoritemurder dataset
- ðŸ”„ Verify S3 upload works with file-based approach
- ðŸ”„ Confirm memory usage improvements

## Key Benefits
- No string length limits (streaming approach)
- Better compression (MsgPack + gzip/zstd)
- Lower memory usage during serialization
- Proven to work with large datasets

## Implementation Notes
- Using MsgPack for binary serialization (more efficient than JSON)
- Streaming prevents large string creation
- Compression reduces final file size significantly
- zstd offers best compression ratio + speed balance